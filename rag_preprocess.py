import torch
import os
from typing import Optional
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_openai import ChatOpenAI

# ===================== é¢„å¤„ç†é…ç½®ï¼ˆä¸chain.pyä¿æŒä¸€è‡´ï¼‰=====================
# APIå¯†é’¥ï¼ˆä»…é¢„å¤„ç†é˜¶æ®µåˆå§‹åŒ–å‹ç¼©å™¨éœ€è¦ï¼Œåç»­ä¸»ç¨‹åºæ— éœ€é‡å¤åˆå§‹åŒ–ï¼‰
DASHSCOPE_API_KEY = ""
QWEN_API_BASE = "https://dashscope.aliyuncs.com/compatible-mode/v1"
# RAGæºæ–‡æ¡£è·¯å¾„
RAG_DOCUMENTS = ["rag/SCL-90.txt", "rag/å¿ƒç†å’¨è¯¢ä¸å¿ƒç†æ²»ç–—.pdf"]
# Embeddingsæ¨¡å‹
EMBEDDINGS_MODEL = "text2vec-base-chinese"
# æ£€ç´¢é…ç½®
RAG_SEARCH_KWARGS = {"k": 3}  # æ£€ç´¢æ–‡æ¡£æ•°é‡
# æœ¬åœ°ä¿å­˜è·¯å¾„ï¼ˆå‘é‡åº“+é…ç½®æ–‡ä»¶ï¼‰
RAG_SAVE_DIR = "./saved_rag"  # å»ºè®®ä¸ä¸»ç¨‹åºåŒç›®å½•ï¼Œæ–¹ä¾¿è°ƒç”¨
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
os.makedirs(RAG_SAVE_DIR, exist_ok=True)


# ===================== å·¥å…·å‡½æ•°ï¼ˆå¤ç”¨chain.pyæ ¸å¿ƒé€»è¾‘ï¼‰=====================
def init_qwen_llm(model_name: str = "qwen-plus", temperature: float = 0.0) -> ChatOpenAI:
    """åˆå§‹åŒ–Qwenæ¨¡å‹ï¼ˆä»…ç”¨äºæ–‡æ¡£å‹ç¼©å™¨ï¼‰"""
    return ChatOpenAI(
        model_name=model_name,
        api_key=DASHSCOPE_API_KEY,
        base_url=QWEN_API_BASE,
        temperature=temperature,
        max_tokens=1024,
        timeout=30,
        verbose=False
    )


def build_and_save_rag() -> Optional[bool]:
    """æ„å»ºRAGå‘é‡åº“å’Œæ£€ç´¢å™¨ï¼Œå¹¶ä¿å­˜åˆ°æœ¬åœ°"""
    try:
        # 1. åŠ è½½æ–‡æ¡£ï¼ˆä¸chain.pyé€»è¾‘ä¸€è‡´ï¼‰
        print("1. åŠ è½½RAGæºæ–‡æ¡£...")
        docs = []
        for doc_path in RAG_DOCUMENTS:
            if not os.path.exists(doc_path):
                print(f"âš ï¸ æ–‡æ¡£ä¸å­˜åœ¨ï¼š{doc_path}ï¼Œå·²è·³è¿‡")
                continue
            try:
                if doc_path.endswith(".txt"):
                    loader = TextLoader(doc_path, encoding='utf-8')
                    docs.extend(loader.load())
                elif doc_path.endswith(".pdf"):
                    loader = PyPDFLoader(doc_path)
                    docs.extend(loader.load())
                print(f"âœ… åŠ è½½æˆåŠŸï¼š{doc_path}ï¼ˆç´¯è®¡{len(docs)}æ®µï¼‰")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å¤±è´¥ {doc_path}ï¼š{str(e)[:50]}...")

        if not docs:
            print("âŒ æœªåŠ è½½åˆ°ä»»ä½•æ–‡æ¡£ï¼Œç»ˆæ­¢é¢„å¤„ç†")
            return False

        # 2. åˆ†å‰²æ–‡æ¡£ï¼ˆä¸chain.pyé€»è¾‘ä¸€è‡´ï¼‰
        print(f"2. åˆ†å‰²æ–‡æ¡£ï¼ˆåŸå§‹æ–‡æ¡£æ•°ï¼š{len(docs)}ï¼‰...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,
            chunk_overlap=30,
            separators=["ã€‚", "ï¼Ÿ", "ï¼", "ï¼›", "\n", "ï¼Œ", " ", ""],
            length_function=len
        )
        splits = text_splitter.split_documents(docs)
        print(f"âœ… åˆ†å‰²å®Œæˆï¼Œå¾—åˆ° {len(splits)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")

        # 3. æ„å»ºEmbeddingså’Œå‘é‡åº“
        print(f"3. åˆå§‹åŒ–Embeddingsæ¨¡å‹ï¼ˆ{EMBEDDINGS_MODEL}ï¼‰...")
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDINGS_MODEL,
            model_kwargs={'device': DEVICE},
            encode_kwargs={'normalize_embeddings': True}
        )

        print("4. æ„å»ºFAISSå‘é‡åº“...")
        db = FAISS.from_documents(splits, embeddings)
        # ä¿å­˜å‘é‡åº“åˆ°æœ¬åœ°ï¼ˆå…³é”®æ­¥éª¤ï¼‰
        vector_db_path = os.path.join(RAG_SAVE_DIR, "faiss_vector_db")
        db.save_local(vector_db_path)
        print(f"âœ… å‘é‡åº“ä¿å­˜æˆåŠŸï¼š{vector_db_path}")

        # 5. æ„å»ºå¸¦å‹ç¼©çš„æ£€ç´¢å™¨ï¼ˆä»…éœ€ä¿å­˜é…ç½®ï¼Œæ— éœ€åºåˆ—åŒ–æ£€ç´¢å™¨æœ¬èº«ï¼‰
        # æ³¨ï¼šæ£€ç´¢å™¨ä¾èµ–LLMå’Œå‘é‡åº“ï¼ŒLLMæ¯æ¬¡å¯åŠ¨éœ€é‡æ–°åˆå§‹åŒ–ï¼Œå› æ­¤ä»…ä¿å­˜å‘é‡åº“+é…ç½®
        print("5. éªŒè¯æ£€ç´¢å™¨é…ç½®...")
        llm = init_qwen_llm(temperature=0.0)
        compressor = LLMChainExtractor.from_llm(llm)
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=db.as_retriever(search_kwargs=RAG_SEARCH_KWARGS)
        )
        # æµ‹è¯•æ£€ç´¢å™¨æœ‰æ•ˆæ€§(ä¸è¦å°±æ³¨é‡Šæ‰å§)
        test_query = "ä»€ä¹ˆæ˜¯SCL-90ï¼Ÿ"
        test_docs = compression_retriever.get_relevant_documents(test_query)
        print(f"âœ… æ£€ç´¢å™¨éªŒè¯æˆåŠŸï¼šæŸ¥è¯¢ã€Œ{test_query}ã€è¿”å› {len(test_docs)} ä¸ªç›¸å…³æ–‡æ¡£")

        # 6. ä¿å­˜æ£€ç´¢å™¨é…ç½®ï¼ˆæ–¹ä¾¿ä¸»ç¨‹åºå¤ç”¨å‚æ•°ï¼‰
        config = {
            "embeddings_model": EMBEDDINGS_MODEL,
            "search_kwargs": RAG_SEARCH_KWARGS,
            "vector_db_path": vector_db_path
        }
        import json
        config_path = os.path.join(RAG_SAVE_DIR, "rag_config.json")
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ£€ç´¢å™¨é…ç½®ä¿å­˜æˆåŠŸï¼š{config_path}")

        print("\nğŸ‰ RAGé¢„å¤„ç†å®Œæˆï¼åç»­ä¸»ç¨‹åºå¯ç›´æ¥åŠ è½½ ./saved_rag ç›®å½•")
        return True

    except Exception as e:
        print(f"âŒ é¢„å¤„ç†å¤±è´¥ï¼š{str(e)}")
        return False


if __name__ == "__main__":
    build_and_save_rag()
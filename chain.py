import torch
import numpy as np
from typing import Tuple, Optional
import os
import json
from langchain.schema import HumanMessage, AIMessage
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, RetrievalQA
from langchain_openai import ChatOpenAI
from transformers import BertTokenizer, BertForSequenceClassification
# RAG ç›¸å…³å¯¼å…¥
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
# å¯¹è¯å†å²ç®¡ç†ç»„ä»¶
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory

# ===================== ã€æ ¸å¿ƒé…ç½®ã€‘ =====================
DASHSCOPE_API_KEY = ""
QWEN_API_BASE = "https://dashscope.aliyuncs.com/compatible-mode/v1"
BERT_MODEL_PATH = "./bert_finetuned/bert_finetuned_20250908_135059"
BERT_DEFAULT_MODEL = "bert-base-chinese"
BERT_MAX_SEQ_LEN = 512  # è°ƒæ•´ä¸ºæ›´å¤§çš„åºåˆ—é•¿åº¦ä»¥å®¹çº³å¤šè½®ç”¨æˆ·è¾“å…¥
MAX_CONVERSATION_ROUNDS = 5
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ç¯å¢ƒå˜é‡æ£€æŸ¥
if not DASHSCOPE_API_KEY or DASHSCOPE_API_KEY.startswith("YOUR_OWN"):
    raise ValueError("è¯·æ›¿æ¢DASHSCOPE_API_KEYä¸ºæœ‰æ•ˆçš„APIå¯†é’¥")

# RAG é…ç½®
RAG_SAVE_DIR = "./saved_rag"
RAG_CONFIG_PATH = os.path.join(RAG_SAVE_DIR, "rag_config.json")


# ===================== RAGå·¥å…·å‡½æ•° =====================
def init_rag() -> Optional[RetrievalQA]:
    """ä»æœ¬åœ°åŠ è½½é¢„æ„å»ºçš„RAGæ£€ç´¢é“¾"""
    try:
        if not os.path.exists(RAG_SAVE_DIR) or not os.path.exists(RAG_CONFIG_PATH):
            print(f"[è­¦å‘Š] æœ¬åœ°RAGæ–‡ä»¶ä¸å­˜åœ¨ï¼ˆ{RAG_SAVE_DIR}ï¼‰ï¼Œè¯·å…ˆè¿è¡Œrag_preprocess.py")
            return None

        # åŠ è½½RAGé…ç½®
        print("1. åŠ è½½æœ¬åœ°RAGé…ç½®...")
        with open(RAG_CONFIG_PATH, "r", encoding="utf-8") as f:
            rag_config = json.load(f)
        embeddings_model = rag_config["embeddings_model"]
        search_kwargs = rag_config["search_kwargs"]
        vector_db_path = rag_config["vector_db_path"]
        print(f"[æˆåŠŸ] é…ç½®åŠ è½½ï¼šEmbeddings={embeddings_model}ï¼Œæ£€ç´¢æ•°é‡={search_kwargs['k']}")

        # åˆå§‹åŒ–Embeddings
        print(f"2. åˆå§‹åŒ–Embeddingsæ¨¡å‹ï¼ˆ{embeddings_model}ï¼‰...")
        embeddings = HuggingFaceEmbeddings(
            model_name=embeddings_model,
            model_kwargs={'device': DEVICE},
            encode_kwargs={'normalize_embeddings': True}
        )

        # åŠ è½½FAISSå‘é‡åº“
        print(f"3. åŠ è½½æœ¬åœ°FAISSå‘é‡åº“...")
        if not os.path.exists(vector_db_path):
            print(f"[è­¦å‘Š] å‘é‡åº“è·¯å¾„ä¸å­˜åœ¨ï¼š{vector_db_path}")
            return None

        db = FAISS.load_local(
            vector_db_path,
            embeddings,
            allow_dangerous_deserialization=True
        )
        print(f"[æˆåŠŸ] å‘é‡åº“åŠ è½½ï¼ˆå« {db.index.ntotal} ä¸ªæ–‡æœ¬ç‰‡æ®µï¼‰")

        # æ„å»ºå‹ç¼©æ£€ç´¢å™¨
        print("4. åˆå§‹åŒ–æ–‡æ¡£å‹ç¼©æ£€ç´¢å™¨...")
        llm = init_qwen_llm(temperature=0.0)
        compressor = LLMChainExtractor.from_llm(llm)
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=db.as_retriever(search_kwargs=search_kwargs)
        )

        # é‡å»ºæ£€ç´¢é“¾
        rag_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=compression_retriever,
            return_source_documents=True,
            verbose=False
        )
        print("[æˆåŠŸ] RAGæ£€ç´¢é“¾åŠ è½½å®Œæˆ")
        return rag_chain

    except Exception as e:
        print(f"[è­¦å‘Š] RAGåŠ è½½å¤±è´¥ï¼š{str(e)}ï¼Œå°†ç»§ç»­è¿è¡Œä½†ä¸ä½¿ç”¨RAGåŠŸèƒ½")
        return None


# ===================== é€šç”¨å·¥å…·å‡½æ•° =====================
def format_conversation(chat_history: InMemoryChatMessageHistory) -> str:
    """æ ¼å¼åŒ–å®Œæ•´å¯¹è¯å†å²ï¼ˆç”¨äºè¯Šæ–­æŠ¥å‘Šï¼‰"""
    try:
        if not chat_history.messages:
            return "æ— æœ‰æ•ˆå¯¹è¯è®°å½•"

        formatted_text = []
        for idx, msg in enumerate(chat_history.messages, 1):
            if isinstance(msg, HumanMessage):
                formatted_text.append(f"ã€ç”¨æˆ·{idx // 2 + 1}ã€‘ï¼š{msg.content.strip()}")
            elif isinstance(msg, AIMessage):
                formatted_text.append(f"ã€åŠ©æ‰‹{idx // 2 + 1}ã€‘ï¼š{msg.content.strip()}")
        return "\n".join(formatted_text)
    except Exception as e:
        print(f"[è­¦å‘Š] æ ¼å¼åŒ–å¯¹è¯å†å²å¤±è´¥ï¼š{str(e)}")
        return "å¯¹è¯å†å²æ ¼å¼åŒ–å¤±è´¥"


def format_user_inputs(chat_history: InMemoryChatMessageHistory) -> str:
    """ä»…æå–å¹¶æ ¼å¼åŒ–ç”¨æˆ·è¾“å…¥ï¼ˆç”¨äºBERTè¯„ä¼°ï¼‰"""
    try:
        if not chat_history.messages:
            return "æ— æœ‰æ•ˆç”¨æˆ·è¾“å…¥"

        user_inputs = []
        for msg in chat_history.messages:
            if isinstance(msg, HumanMessage):
                content = msg.content.strip()
                if content.lower() not in ["ç»“æŸ", "åœæ­¢", "é€€å‡º"]:
                    user_inputs.append(f"ã€ç”¨æˆ·{len(user_inputs) + 1}ã€‘ï¼š{content}")

        # ç¡®ä¿æœ€å¤šä¿ç•™5è½®ç”¨æˆ·è¾“å…¥
        return "\n".join(user_inputs[:MAX_CONVERSATION_ROUNDS])
    except Exception as e:
        print(f"[è­¦å‘Š] æå–ç”¨æˆ·è¾“å…¥å¤±è´¥ï¼š{str(e)}")
        return "ç”¨æˆ·è¾“å…¥æå–å¤±è´¥"


def validate_user_inputs(text: str) -> bool:
    """éªŒè¯ç”¨æˆ·è¾“å…¥æœ‰æ•ˆæ€§ï¼ˆç”¨äºBERTè¯„ä¼°ï¼‰"""
    return len(text.strip()) >= 30 and "ã€ç”¨æˆ·" in text


# ===================== LLMåˆå§‹åŒ–å‡½æ•° =====================
def init_qwen_llm(model_name: str = "qwen-plus", temperature: float = 0.4) -> ChatOpenAI:
    """åˆå§‹åŒ–Qwenæ¨¡å‹"""
    return ChatOpenAI(
        model_name=model_name,
        api_key=DASHSCOPE_API_KEY,
        base_url=QWEN_API_BASE,
        temperature=temperature,
        max_tokens=1024,
        timeout=30,
        verbose=False
    )


# ===================== å¯¹è¯äº¤äº’å‡½æ•° =====================
def run_llm_conversation(rag_chain) -> Tuple[InMemoryChatMessageHistory, str, str]:
    """å¸¦RAGæ”¯æŒçš„å¯¹è¯äº¤äº’ï¼Œè¿”å›å®Œæ•´å†å²ã€æ ¼å¼åŒ–å†å²ã€çº¯ç”¨æˆ·è¾“å…¥"""
    print("\n=== ç²¾ç¥å¥åº·ä¿¡æ¯æ”¶é›†ï¼ˆè¾“å…¥â€œç»“æŸâ€åœæ­¢å¯¹è¯ï¼‰===")
    print(f"æç¤ºï¼šå…±æ”¯æŒ {MAX_CONVERSATION_ROUNDS} è½®å¯¹è¯ï¼Œè¯·å°½é‡è¯¦ç»†æè¿°æƒ…å†µ\n")

    # åˆå§‹åŒ–LLMå’Œå¯¹è¯å†å²
    chat_llm = init_qwen_llm(model_name="qwen-plus", temperature=0.4)
    chat_history = InMemoryChatMessageHistory()

    # å®šä¹‰Promptæ¨¡æ¿
    prompt_template = PromptTemplate(
        input_variables=["history", "input", "docs"],
        template="""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢è¾…åŠ©å¸ˆï¼Œä»…æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
1. æ ¸å¿ƒè§„åˆ™ï¼šæ¯æ¬¡æ1-2ä¸ªé—®é¢˜æ”¶é›†ç”¨æˆ·ä¿¡æ¯ï¼ˆèšç„¦æƒ…ç»ª/èº«ä½“ç—‡çŠ¶/è¡Œä¸ºå˜åŒ–ï¼‰ï¼Œä¸æå‰è¯Šæ–­ã€ä¸ç»™å‡ºå»ºè®®ï¼›
2. ç»“æŸè§„åˆ™ï¼šè‹¥ç”¨æˆ·è¾“å…¥â€œç»“æŸâ€ï¼Œç«‹å³å›å¤â€œå¯¹è¯å·²ç»“æŸï¼Œå°†ä¸ºä½ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šâ€ï¼Œæ— éœ€é¢å¤–å†…å®¹ï¼›
3. å‚è€ƒè§„åˆ™ï¼šå¯å‚è€ƒæ–‡æ¡£ä¿¡æ¯ï¼ˆéå¿…é¡»ï¼‰ï¼Œä½†é—®é¢˜éœ€åŸºäºç”¨æˆ·å½“å‰è¾“å…¥è®¾è®¡ï¼Œä¸è„±ç¦»ä¸Šä¸‹æ–‡ã€‚

å‚è€ƒæ–‡æ¡£ï¼š
{docs}

å¯¹è¯å†å²ï¼š
{history}

ç”¨æˆ·å½“å‰è¾“å…¥ï¼š
{input}

ä½ çš„å›å¤ï¼ˆä»…1-2ä¸ªé—®é¢˜æˆ–ç»“æŸæç¤ºï¼Œä¸è¶…è¿‡3å¥è¯ï¼Œä¸­æ–‡è¡¨è¾¾ï¼‰ï¼š
"""
    )

    # åˆ›å»ºå¸¦å†å²çš„å¯¹è¯é“¾
    base_chain = prompt_template | chat_llm
    conversation_chain = RunnableWithMessageHistory(
        base_chain,
        lambda session_id: chat_history,
        input_messages_key="input",
        history_messages_key="history",
        session_id="mental_health_chat_001"
    )

    # å¤šè½®å¯¹è¯å¾ªç¯
    for round_idx in range(MAX_CONVERSATION_ROUNDS):
        try:
            user_input = input(f"\nã€ç¬¬{round_idx + 1}è½®/å…±{MAX_CONVERSATION_ROUNDS}è½®ã€‘è¯·è¾“å…¥ä½ çš„æƒ…å†µï¼š").strip()
            if not user_input:
                print("ã€åŠ©æ‰‹ã€‘ï¼šè¯·æè¿°ä½ çš„å…·ä½“æƒ…å†µï¼ˆä¾‹å¦‚ï¼šæœ€è¿‘æ˜¯å¦ç»å¸¸å¤±çœ ï¼Ÿæƒ…ç»ªæ˜¯å¦å®¹æ˜“ä½è½ï¼Ÿï¼‰")
                continue

            # å¤„ç†ç»“æŸæŒ‡ä»¤
            if user_input.lower() in ["ç»“æŸ", "åœæ­¢", "é€€å‡º"]:
                print("ã€åŠ©æ‰‹ã€‘ï¼šå¯¹è¯å·²ç»“æŸï¼Œå°†ä¸ºä½ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
                chat_history.add_user_message(user_input)
                chat_history.add_ai_message("å¯¹è¯å·²ç»“æŸï¼Œå°†ä¸ºä½ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
                break

            # æ£€ç´¢RAGæ–‡æ¡£
            docs_content = "æ— å‚è€ƒæ–‡æ¡£"
            if rag_chain:
                try:
                    print(f"[æœç´¢ä¸­] æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆç¬¬{round_idx + 1}è½®ï¼‰...")
                    rag_result = rag_chain.invoke({"query": user_input})
                    docs_content = "\n".join([
                        f"æ–‡æ¡£ç‰‡æ®µ{idx + 1}ï¼š{doc.page_content.strip()[:200]}..."
                        for idx, doc in enumerate(rag_result["source_documents"])
                    ])
                except Exception as e:
                    print(f"[è­¦å‘Š] æ–‡æ¡£æ£€ç´¢å‡ºé”™ï¼š{str(e)[:30]}...ï¼Œå°†åŸºäºå¯¹è¯å†å²æé—®")
                    docs_content = "æ–‡æ¡£æ£€ç´¢å¤±è´¥ï¼Œæ— å‚è€ƒä¿¡æ¯"

            # è°ƒç”¨LLMè·å–å›å¤
            response = conversation_chain.invoke(
                {"input": user_input, "docs": docs_content},
                config={"configurable": {"session_id": "mental_health_chat_001"}}
            )

            # è§£æå›å¤å¹¶æ›´æ–°å†å²
            llm_response = response.content.strip() if hasattr(response, "content") else str(response)
            print(f"ã€åŠ©æ‰‹ã€‘ï¼š{llm_response}")

        except KeyboardInterrupt:
            raise ValueError("ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­å¯¹è¯")
        except Exception as e:
            error_msg = str(e)
            if "Invalid API key" in error_msg:
                raise ValueError("APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥DASHSCOPE_API_KEYæ˜¯å¦æ­£ç¡®")
            elif "NetworkError" in error_msg:
                print("ã€åŠ©æ‰‹ã€‘ï¼šå½“å‰ç½‘ç»œä¸ç¨³å®šï¼Œè¯·é‡è¯•ä¸€æ¬¡")
                continue
            else:
                print(f"ã€åŠ©æ‰‹ã€‘ï¼šå¯¹è¯å‡ºé”™ï¼š{error_msg[:50]}...ï¼Œè¯·é‡è¯•")
                continue

    # ç”Ÿæˆä¸¤ç§æ ¼å¼åŒ–ç»“æœ
    full_history = format_conversation(chat_history)
    user_inputs_text = format_user_inputs(chat_history)

    # éªŒè¯ç”¨æˆ·è¾“å…¥æœ‰æ•ˆæ€§
    if not validate_user_inputs(user_inputs_text):
        raise ValueError(f"ç”¨æˆ·è¾“å…¥æ— æ•ˆï¼ˆå†…å®¹ï¼š{user_inputs_text[:50]}...ï¼‰ï¼Œæ— æ³•è¿›è¡ŒBERTè¯„ä¼°")

    return chat_history, full_history, user_inputs_text


# ===================== BERTç›¸å…³å‡½æ•° =====================
def load_bert_model() -> Tuple[BertTokenizer, BertForSequenceClassification]:
    """åŠ è½½BERTæ¨¡å‹"""
    print(f"\n3. å¼€å§‹åŠ è½½BERTæ¨¡å‹ï¼ˆè®¾å¤‡ï¼š{DEVICE}ï¼‰...")
    try:
        # ä¼˜å…ˆåŠ è½½æœ¬åœ°æ¨¡å‹
        if os.path.exists(BERT_MODEL_PATH) and len(os.listdir(BERT_MODEL_PATH)) > 3:
            tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH)
            model = BertForSequenceClassification.from_pretrained(
                BERT_MODEL_PATH,
                num_labels=2,
                ignore_mismatched_sizes=True
            )
            model = model.to(DEVICE)
            print(f"[æˆåŠŸ] åŠ è½½æœ¬åœ°BERTæ¨¡å‹ï¼š{BERT_MODEL_PATH}ï¼ˆè®¾å¤‡ï¼š{DEVICE}ï¼‰")
            return tokenizer, model
        else:
            print(f"[è­¦å‘Š] æœ¬åœ°æ¨¡å‹è·¯å¾„æ— æ•ˆï¼ˆ{BERT_MODEL_PATH}ï¼‰ï¼Œå°†åŠ è½½é»˜è®¤æ¨¡å‹ï¼š{BERT_DEFAULT_MODEL}")
    except Exception as e:
        print(f"[è­¦å‘Š] æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)[:50]}...ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡å‹")

    # åŠ è½½é»˜è®¤BERTæ¨¡å‹
    try:
        tokenizer = BertTokenizer.from_pretrained(BERT_DEFAULT_MODEL)
        model = BertForSequenceClassification.from_pretrained(
            BERT_DEFAULT_MODEL,
            num_labels=2
        )
        model = model.to(DEVICE)
        print(f"[æˆåŠŸ] åŠ è½½é»˜è®¤BERTæ¨¡å‹ï¼š{BERT_DEFAULT_MODEL}ï¼ˆè®¾å¤‡ï¼š{DEVICE}ï¼‰")
        return tokenizer, model
    except Exception as e:
        raise ValueError(f"BERTæ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}ï¼ˆè¯·ç¡®ä¿ç½‘ç»œé€šç•…ï¼‰")


def bert_classify(text: str, tokenizer: BertTokenizer, model: BertForSequenceClassification) -> Tuple[int, float]:
    """ä½¿ç”¨BERTè¿›è¡Œåˆ†ç±»"""
    print("\n4. å¼€å§‹BERTç²¾ç¥å¥åº·é£é™©åˆ†ç±»...")
    try:
        # æ–‡æœ¬ç¼–ç 
        inputs = tokenizer(
            text,
            max_length=BERT_MAX_SEQ_LEN,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        ).to(DEVICE)

        # æ¨¡å‹æ¨ç†
        model.eval()
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=-1).cpu().numpy()[0]

        # è§£æç»“æœ
        predicted_label = int(np.argmax(probabilities))
        confidence = round(float(probabilities[predicted_label]), 4)
        label_desc = "å­˜åœ¨ç²¾ç¥å¥åº·é—®é¢˜é£é™©" if predicted_label == 1 else "æ— ç²¾ç¥å¥åº·é—®é¢˜é£é™©"

        # æ‰“å°è¯¦ç»†ç»“æœ
        print(f" BERTåˆ†ç±»ç»“æœï¼š")
        print(f"   - é£é™©åˆ¤æ–­ï¼š{label_desc}")
        print(f"   - é¢„æµ‹æ ‡ç­¾ï¼š{predicted_label}ï¼ˆ0=æ— é£é™©ï¼Œ1=æœ‰é£é™©ï¼‰")
        print(f"   - ç½®ä¿¡åº¦ï¼š{confidence * 100:.2f}%")
        return predicted_label, confidence

    except Exception as e:
        raise ValueError(f"BERTåˆ†ç±»å¤±è´¥ï¼š{str(e)}")


# ===================== è¯Šæ–­å»ºè®®ç”Ÿæˆå‡½æ•° =====================
def generate_diagnosis(history_text: str, bert_label: int, confidence: float, rag_chain) -> str:
    """ç”Ÿæˆå¸¦RAGæ”¯æŒçš„è¯Šæ–­å»ºè®®"""
    print("\n5. å¼€å§‹ç”Ÿæˆç²¾ç¥å¥åº·è¯„ä¼°æŠ¥å‘Š...")
    # åˆå§‹åŒ–è¯Šæ–­ç”¨LLM
    diagnosis_llm = init_qwen_llm(model_name="qwen-plus", temperature=0.3)
    bert_result = "å­˜åœ¨ç²¾ç¥å¥åº·é—®é¢˜é£é™©" if bert_label == 1 else "æ— ç²¾ç¥å¥åº·é—®é¢˜é£é™©"

    # æ£€ç´¢è¯Šæ–­ç›¸å…³æ–‡æ¡£
    docs_content = "æ— å‚è€ƒæ–‡æ¡£"
    if rag_chain:
        try:
            print("[æœç´¢ä¸­] æ­£åœ¨æ£€ç´¢è¯Šæ–­å‚è€ƒæ–‡æ¡£...")
            rag_result = rag_chain.invoke({"query": f"åŸºäºå¯¹è¯å†å²çš„ç²¾ç¥å¥åº·è¯„ä¼°ï¼š{history_text[:200]}..."})
            docs_content = []
            for idx, doc in enumerate(rag_result["source_documents"], 1):
                source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
                page = doc.metadata.get("page", "æœªçŸ¥é¡µç ")
                content = doc.page_content.strip()[:300] + "..."
                docs_content.append(f"å‚è€ƒæ–‡æ¡£{idx}ï¼ˆ{source} ç¬¬{page}é¡µï¼‰ï¼š{content}")
            docs_content = "\n\n".join(docs_content)
        except Exception as e:
            print(f"[è­¦å‘Š] è¯Šæ–­é˜¶æ®µæ–‡æ¡£æ£€ç´¢å‡ºé”™ï¼š{str(e)}ï¼Œå°†åŸºäºç°æœ‰ä¿¡æ¯ç”ŸæˆæŠ¥å‘Š")
            docs_content = "æ–‡æ¡£æ£€ç´¢å¤±è´¥ï¼ŒåŸºäºå¯¹è¯å†å²å’ŒBERTç»“æœç”ŸæˆæŠ¥å‘Š"

    confidence_percent = confidence * 100

    # å®šä¹‰è¯Šæ–­æŠ¥å‘ŠPrompt
    diagnosis_prompt = PromptTemplate(
        input_variables=["history", "bert_result", "confidence_percent", "docs"],
        template="""ä½ æ˜¯ä¸“ä¸šçš„ç²¾ç¥å¥åº·è¯„ä¼°è¾…åŠ©é¡¾é—®ï¼ŒåŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆç»“æ„åŒ–è¯„ä¼°æŠ¥å‘Šï¼Œè¯­è¨€æ¸©å’Œã€æ˜“æ‡‚ã€‚

ã€æ ¸å¿ƒä¾æ®ã€‘
1. ç”¨æˆ·å¯¹è¯å†å²ï¼ˆç²¾ç®€ç‰ˆï¼‰ï¼š
{history}

2. BERTæ¨¡å‹è¯„ä¼°ç»“æœï¼š
{bert_result}ï¼ˆç½®ä¿¡åº¦ï¼š{confidence_percent:.2f}%ï¼‰

3. å‚è€ƒæ–‡æ¡£ä¿¡æ¯ï¼š
{docs}

ã€è¾“å‡ºè¦æ±‚ã€‘
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹3ä¸ªæ¨¡å—è¾“å‡ºï¼Œæ¯ä¸ªæ¨¡å—2-5å¥è¯ï¼Œä¸è¶…è¿‡600å­—ï¼š
1. ã€è¯Šæ–­ç»“è®ºã€‘ï¼šç»“åˆå¯¹è¯ä¸­çš„å…·ä½“ç—‡çŠ¶ï¼Œè¯´æ˜æ˜¯å¦å­˜åœ¨é£é™©åŠé£é™©ç±»å‹ï¼›
2. ã€è¯„ä¼°å»ºè®®ã€‘ï¼šç»™å‡ºå…·ä½“çš„è‡ªæˆ‘è¯„ä¼°å»ºè®®ï¼›
3. ã€å°±åŒ»å»ºè®®ã€‘ï¼šè‹¥å­˜åœ¨é£é™©ï¼Œç»™å‡ºå…·ä½“çš„å°±åŒ»æŒ‡å¼•ï¼›è‹¥æ— é£é™©ï¼Œç»™å‡ºå¥åº·ç»´æŠ¤å»ºè®®ã€‚

ã€ç¦æ­¢å†…å®¹ã€‘
- ä¸æ¨èå…·ä½“è¯ç‰©æˆ–æ²»ç–—æ–¹æ¡ˆï¼›
- ä¸ä½¿ç”¨â€œç¡®è¯Šâ€â€œä¸€å®šâ€ç­‰ç»å¯¹åŒ–è¡¨è¿°ï¼›
- ä¸æ‰©å±•æ— å…³å†…å®¹ã€‚

è¯·ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šï¼š
"""
    )

    # è°ƒç”¨LLMç”ŸæˆæŠ¥å‘Š
    diagnosis_chain = LLMChain(
        llm=diagnosis_llm,
        prompt=diagnosis_prompt,
        verbose=False
    )
    result = diagnosis_chain.invoke({
        "history": history_text[:1000],
        "bert_result": bert_result,
        "confidence_percent": confidence_percent,
        "docs": docs_content
    })

    return result.get("text", "è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•")


# ===================== ä¸»æµç¨‹å‡½æ•° =====================
def main():
    print("=" * 70)
    print(f"ç²¾ç¥å¥åº·è¯„ä¼°ç³»ç»Ÿï¼ˆ{DEVICE}ç‰ˆï¼‰")
    print("=" * 70)
    print(f"å½“å‰é…ç½®ï¼šQwen-Plus LLM / BERTåˆ†ç±» / æœ¬åœ°RAGæ£€ç´¢")
    print("=" * 70 + "\n")

    try:
        # æ­¥éª¤1ï¼šåŠ è½½æœ¬åœ°RAGæ£€ç´¢é“¾
        print("ã€æ­¥éª¤1/4ã€‘åŠ è½½æœ¬åœ°æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿ...")
        rag_chain = init_rag()

        # æ­¥éª¤2ï¼šå¤šè½®å¯¹è¯æ”¶é›†ä¿¡æ¯
        print("\nã€æ­¥éª¤2/4ã€‘å¼€å§‹å¯¹è¯ä¿¡æ¯æ”¶é›†...")
        chat_history, full_history, user_inputs_text = run_llm_conversation(rag_chain)
        print(f"\nğŸ“ æ•´ç†åçš„ç”¨æˆ·è¾“å…¥ï¼š\n{user_inputs_text}")

        # æ­¥éª¤3ï¼šBERTæ¨¡å‹åˆ†ç±»è¯„ä¼°ï¼ˆä»…ä½¿ç”¨ç”¨æˆ·è¾“å…¥ï¼‰
        print("\nã€æ­¥éª¤3/4ã€‘å¼€å§‹BERTé£é™©è¯„ä¼°...")
        bert_tokenizer, bert_model = load_bert_model()
        bert_label, bert_confidence = bert_classify(user_inputs_text, bert_tokenizer, bert_model)

        # æ­¥éª¤4ï¼šç”Ÿæˆæœ€ç»ˆè¯„ä¼°æŠ¥å‘Šï¼ˆä½¿ç”¨å®Œæ•´å†å²ï¼‰
        print("\nã€æ­¥éª¤4/4ã€‘ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        final_diagnosis = generate_diagnosis(full_history, bert_label, bert_confidence, rag_chain)

        # è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
        print("\n" + "=" * 80)
        print("ç²¾ç¥å¥åº·è¯„ä¼°æŠ¥å‘Šï¼ˆè¾…åŠ©ç‰ˆï¼‰")
        print("=" * 80)
        print(final_diagnosis)
        print("=" * 80)
        print("\nã€é‡è¦æç¤ºã€‘æœ¬æŠ¥å‘Šä»…ä¸ºAIè¾…åŠ©è¯„ä¼°ï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿè¯Šæ–­ï¼")
        print("è‹¥å­˜åœ¨æ˜æ˜¾ä¸é€‚ï¼Œè¯·åŠæ—¶å‰å¾€æ­£è§„åŒ»ç–—æœºæ„å°±è¯Šã€‚")
        print("=" * 80)

    except Exception as e:
        print(f"\n[Error!] æµç¨‹è¿è¡Œå¤±è´¥ï¼š{str(e)}")
        print("å»ºè®®æ£€æŸ¥ï¼š1. APIå¯†é’¥æœ‰æ•ˆæ€§ 2. æœ¬åœ°RAGèµ„æºå®Œæ•´æ€§ 3. ç½‘ç»œè¿æ¥")


if __name__ == "__main__":
    main()